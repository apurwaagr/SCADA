{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Playground\n",
    "\n",
    "This notebook is accompanying the Ingenuity Blog post: https://blog.siemens.com/2023/07/build-your-own-semantic-search-with-large-language-models/\n",
    "\n",
    "![alt text](robot_searching_documents.png \"Robot searching documents\")\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain tiktoken faiss-cpu PyPDF2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dependencies and API key. See https://wiki.siemens.com/display/en/The+FAQ+of+ai+attack#TheFAQofaiattack-SiemensOpenAIPlayground on the details how to get access to the Siemens Azure OpenAI endpoint. Your API key needs to be in the file `.key`. In general, make sure to never check the key into version control. This is why `.key` is in `.gitignore`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"PUT YOUR API ENDPOINT HERE\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "with open('.key', 'r') as file:\n",
    "    openai.api_key = file.read().rstrip()\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import urllib\n",
    "import PyPDF2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare semantic index\n",
    "\n",
    "In this section, we divide the large text into chunks. Next, the chunks are embedded. As this takes a while, we will save the embeddings, so next time we can directly load them. This preparation has only to be done the first time you run the notebook.\n",
    "\n",
    "You can bring your own documents. Just put the PDFs into `data/`. For the example we use the [Siemens S7-1500 manual](https://support.industry.siemens.com/cs/attachments/86140384/s71500_et200mp_manual_collection_en-US.pdf). Let's download it! Grab some coffee, the manual is big ;-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download at 100.0%\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('data/step7-manual.pdf', <http.client.HTTPMessage at 0x15e53a2dd10>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare progressbar\n",
    "def show_progress(block_num, block_size, total_size):\n",
    "    print(f'download at {round(block_num * block_size / total_size *100,2)}%', end=\"\\r\")\n",
    "\n",
    "# download the the S7-1500 manual\n",
    "urllib.request.urlretrieve('https://support.industry.siemens.com/cs/attachments/86140384/s71500_et200mp_manual_collection_en-US.pdf', 'data/s7-1500-manual.pdf', show_progress)\n",
    "urllib.request.urlretrieve('https://support.industry.siemens.com/cs/attachments/109742272/STEP_7_Professional_V14_enUS_en-US.pdf', 'data/step7-manual.pdf', show_progress)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that extracts all text from the pdf at `path`\n",
    "def load_pdf_as_string(path:str) -> str:  \n",
    "    \n",
    "    # creating a pdf file object\n",
    "    pdfFileObj = open(path, 'rb')\n",
    "    \n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "\n",
    "    total_pages = len( pdfReader.pages)\n",
    "    pages = []\n",
    "    for i, page in enumerate(pdfReader.pages):\n",
    "        print(f'{path}: {round((i * 100)/total_pages)}% at page: {i}         ', end=\"\\r\")\n",
    "        pages.append(page.extract_text())\n",
    "\n",
    "    text = '\\n\\n'.join(pages)\n",
    "    pdfFileObj.close()\n",
    "    print('')\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create chunks\n",
    "First, we extract the text from the pdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\\s7-1500-manual.pdf: 100% at page: 11924\n",
      "input\\step7-manual.pdf: 100% at page: 14417\n"
     ]
    }
   ],
   "source": [
    "# go through all pdfs and extract the text\n",
    "texts = [load_pdf_as_string(path) for path in glob.glob('input/*.pdf')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the text into chunks and save them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the texts into chunks which are saved into `data/chunks`.\n",
    "CHUNK_SIZE = 15_000\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a chunk size that is about half the size of the LLMs context length. \n",
    "    # We need the rest for the question and the answer.\n",
    "    chunk_size = CHUNK_SIZE,\n",
    "    chunk_overlap  = 5_000,\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents(texts)\n",
    "        \n",
    "for i, doc in enumerate(docs):\n",
    "    with open( f'data/chunks/chunk-{i}.txt',\"w\", encoding=\"utf-8\") as out_page:\n",
    "            out_page.write(doc.page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed chunks\n",
    "\n",
    "Next, we calculate the embedding for each chunk. The embeddings are saved for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key, model_kwargs={\"engine\": \"text-embedding-ada-002\"})\n",
    "embedding_list = []\n",
    "\n",
    "# read all chunks and calculate embedding\n",
    "for i, path in enumerate(glob.glob('data/chunks/chunk-*.txt')):\n",
    "    with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "        chunk = file.read()\n",
    "        # The API does not yet support embedding of multiple texts in one call. Thus the awkward looping and indexing. \n",
    "        embedding_vector = embeddings.embed_documents([chunk], chunk_size=CHUNK_SIZE)\n",
    "        embedding_list.append(embedding_vector[0])\n",
    "\n",
    "\n",
    "with open('data/embeddings.json', 'w') as outfile:\n",
    "    json.dump(embedding_list, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's search!\n",
    "\n",
    "Almost ready.\n",
    "\n",
    "### Load the index\n",
    "\n",
    "We load the embeddings and their respective chunks of text into the [FAISS](https://github.com/facebookresearch/faiss) vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding model\n",
    "embeddings_api = OpenAIEmbeddings(openai_api_key=openai.api_key, model_kwargs={\"engine\": \"text-embedding-ada-002\"})\n",
    "\n",
    "# load the embeddings\n",
    "with open(f'data/embeddings.json') as json_file:\n",
    "    embeddings = json.load(json_file)\n",
    "\n",
    "# load the text chunks\n",
    "chunks = []\n",
    "for path in glob.glob(f'data/chunks/chunk-*.txt'):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        chunks.append(file.read())\n",
    "\n",
    "# setup the vectorstore\n",
    "db = FAISS.from_embeddings(\n",
    "    list(zip(chunks, embeddings)),\n",
    "    embedding=embeddings_api,\n",
    ")\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# prepare the promt template. It takes the question and the chunk of text that hopefully contains the information to answer it\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Answer the following question based on the document. If the document does not provide the information needed, tell so:\\nQUESTION:{question}\\nDOCUMENT:\\n{document}\\n\\nANSWER:\"\n",
    ")\n",
    "\n",
    "# create the llm \n",
    "model_name = \"text-davinci-003\"\n",
    "temperature = 0.0\n",
    "model_api = OpenAI(\n",
    "    model_kwargs={\"engine\": model_name},\n",
    "    temperature=temperature,\n",
    "    max_tokens=500,\n",
    "    openai_api_key=openai.api_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an answer function which takes a question, searches the vectorstore for the closes chunk of text and finally prompts the LLM to answer the question based on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question):\n",
    "    chunks = retriever.get_relevant_documents(question)\n",
    "    # we configured the retriever to give only the best match\n",
    "    chunk = chunks[0]\n",
    "\n",
    "    # fill in the promt template\n",
    "    prompt_value = prompt.format_prompt(\n",
    "                document=chunk.page_content, \n",
    "                question=question\n",
    "            )\n",
    "    try:\n",
    "        print(model_api(prompt_value.to_string()))\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to answer questions. Here a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To trace a signal, double-click the \"Add new trace\" entry in the project tree with the \"Traces\" system folder below the device. Adapt the name of the trace configuration by clicking the text. Select the signals to be recorded in the \"Signals\" area. Configure the sampling, trigger mode and the condition for the selected trigger. Transfer the trace configuration to the device with the button. Activate the recording by clicking the button. Wait until the \"Recording\" or \"Recording completed\" status is displayed in the status display of the trace. Switch to the \"Diagram\" tab and click the icon of a signal in the signal table. Select or deselect the individual signals and bits for display with the icon. Transfer the measurement to the project with the button.\n"
     ]
    }
   ],
   "source": [
    "answer('how do I trace a signal?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ladder diagrams are a graphical representation of a program, while SCL code is a textual representation of a program.\n"
     ]
    }
   ],
   "source": [
    "answer('what is the difference between ladder diagrams and scl code?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system will answer questions only based on the provided PDFs. Let's try a question that cannot be answered based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This document does not provide information about whether or not you should buy Siemens stock.\n"
     ]
    }
   ],
   "source": [
    "answer('Should I buy Siemens stock?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, the system does not fantasize!\n",
    "\n",
    "## Your turn to tinker around\n",
    "\n",
    "Feel free to play around with the parameters of the llm and embeddings api call! \n",
    "\n",
    "Here are a few more things to try:\n",
    "\n",
    "* can you change the prompt to get answers more suitable for novice Simatic users?\n",
    "* is there any other way to improve the prompt to the LLM?\n",
    "* what about getting more than one chunk from the vectorstore? Is there a way to use them to improve the answers?\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
